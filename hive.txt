create database if not exists kalyan
comment 'database name is kalyan'
location '/hive/kalyan'
with dbproperties('key1'='value1')
;


use kalyan ;

create table if not exists student1(name string,id int,coursr string,year int)
row  format delimited 
fields terminated by '\t' 
lines terminated by '\n' 
stored as textfile 
;

create table if not exists student2(name string,id int,coursr string,year int)
row  format delimited 
fields terminated by '\t' 
lines terminated by '\n' 
stored as textfile 
;
create external table if not exists student3(name string,id int,coursr string,year int)
row  format delimited 
fields terminated by '\t' 
lines terminated by '\n' 
stored as textfile 
;
create temporary table if not exists student4(name string,id int,coursr string,year int)
row  format delimited 
fields terminated by '\t' 
lines terminated by '\n' 
stored as textfile 
;

load data local inpath '/home/orienit/student.txt' overwrite into table student1 ;

load data local inpath '/home/orienit/student.txt' overwrite into table student2 ;


load data local inpath '/home/orienit/student.txt' overwrite into table student2 ;

load data local inpath '/home/orienit/student.txt'  into table student2 ;

load data inpath '/hive/kalyan/student1/student.txt' into table student3 ;

dfs -mkdir -p /home/orienit/;
dfs -put /home/orienit/student.txt /home/orienit ;

dfs -ls /home/orienit ;

load data inpath '/home/orienit/student.txt' overwrite into table student1 ;

dfs -ls /home/orienit ;

mysql -u root -phadoop

show databases ;

use hive_metastore_db ;

select * from dbs ;

select * from tbls ;

create table if not exists studenttext1(name string,id int,coursr string,year int)
row format delimited 
fields terminated by '\t' 
lines terminated by '\n' 
stored as textfile ;

select * from studenttext1 ;

dfs -mkdir -p /hive/externaldata ;

dfs -put /home/orienit/student.txt /hive/externaldata ;

create external table if not exists studenttext2(name string,id int,course string,year int)
row format delimited 
fields terminated by '\t' 
lines terminated by '\n'
stored as textfile 
location '/hive/externaldata' 
;

select * from studenttext2 ;

hive (kalyan)> dfs -ls /hive/kalyan ;
Found 4 items
drwxr-xr-x   - orienit supergroup          0 2020-04-16 10:23 /hive/kalyan/student1
drwxr-xr-x   - orienit supergroup          0 2020-04-16 10:19 /hive/kalyan/student2
drwxr-xr-x   - orienit supergroup          0 2020-04-16 10:20 /hive/kalyan/student3
drwxr-xr-x   - orienit supergroup          0 2020-04-16 10:32 /hive/kalyan/studenttext1
hive (kalyan)>

create table if not exists studentdata
(name string comment 'student name',
subjects array<string> comment 'student subjects',
marks map<string,int> comment 'student marks',
address struct<loc:string comment 'student location',pincode:int comment 'student pincode',city:string comment 'student city'> comment 'student address',
details uniontype<double,int,array<string>,struct<a:int,b:string>,double> comment 'student details'
)
comment 'studentdata is the table name'
row format delimited 
fields terminated by '#'
collection items terminated by ','
map keys terminated by ':'
lines terminated by '\n' 
stored as textfile 
;

load data local inpath '/home/orienit/studentdata.txt' overwrite into table studentdata ;

select * from studentdata ;


select name,subjects[0],marks['math'],address.pincode,details from studentdata ;

insert overwrite local directory '/home/orienit/r1' row format delimited fields terminated by '#' collection items terminated by ',' map keys terminated by ':' lines terminated by '\n' select * from studentdata ;

insert overwrite directory '/home/orienit/siva' select name,subjects[0],marks['math'],address.pincode,details from studentdata ;

create table if not exists student
(name string comment 'student name',
id int comment 'student id',
course string comment 'student course',
year int comment 'student year'
)
comment 'student is the table name '
row format delimited 
fields terminated by '\t' 
lines terminated by '\n' 
stored as textfile   
location '/hive/kalyan/student'
tblproperties('key1'='value1')
;
load data local inpath '/home/orienit/student.txt' overwrite into table student ;

select * from student ;

create table if not  exists student_partition(name string,id int)
partitioned by (course string,year int)
row format delimited 
fields terminated by '\t' 
lines terminated by '\n' 
stored as textfile 
location '/hive/kalyan/student_partition' 
;

load data local inpath '/home/orienit/student_cse_1.txt' overwrite into table student_partition partition(course='cse',year=1);

load data local inpath '/home/orienit/student_cse_2.txt' overwrite into table student_partition partition(course='cse',year=2);

set hive.cli.print.current.db=true ;

set hive.cli.print.header=true ;

set hive.exec.dynamic.partition.mode=nonstrict ;

insert into table student_partition partition(course,year) select * from student ;

insert into table student_partition partition(course='it',year=1) select name,id from student where course='cse' and year=1 ; 

insert into table student_partition partition(course="mech1",year) select name,id,year from student where course="cse" ;

create table if not exists users(name string,id int)
row format delimited 
fields terminated by '\t' 
lines terminated by '\n' 
stored as textfile 
location '/hive/kalyan/users' 
;

load data local inpath '/home/orienit/users.txt' overwrite into table users ;

select * from users ;

create table if not exists users1(name string,id int) 
clustered by (id) into 4 buckets 
row format delimited 
fields terminated by '\t' 
lines terminated by '\n' 
stored as textfile ;

insert into table users1 select * from users ;

select * from users1 tablesample(bucket 1 out of 4 on id) ; 

output : 4,8

select * from users1 tablesample(bucket 2 out of 4 on id) ;

output: 1,5,9

create table if not exists users2(name string,id int) 
clustered by (id) sorted by (id desc) into 5 buckets 
row format delimited 
fields terminated by '\t' 
lines terminated by '\n' 
stored as textfile ;


insert overwrite table users2 select * from users ;

select * from users2 tablesample (bucket 3 out of 5 on id) ;

select * from users2 tablesample (bucket 5 out of 5 on id) ;

create table if not exists docs(line string) 
stored as textfile 
;

load data local inpath '/home/orienit/demoinput' overwrite into table docs ;

select * from docs ;

select word,count(*) from (select explode(split(line,' ')) as word  from docs) as w group by word order by word desc;

insert overwrite local directory '/home/orienit/wordcountoutputhive' select word,count(*) from (select explode(split(line,' ')) as word  from docs) as w group by word order by word desc;

create table if not exists products(name string,id int,price bigint)
row format delimited 
fields terminated by '\t' 
lines terminated by '\n'
stored as textfile 
location '/hive/kalyan/products'
;

load data local inpath '/home/orienit/products.txt' overwrite into table products ;

create table if not exists sales(name string,year int,percentage double)
row format delimited 
fields terminated by ',' 
lines terminated by '\n'
stored as textfile 
location '/hive/kalyan/sales'
;

load data local inpath '/home/orienit/sales.txt' overwrite into table sales ;

select products.*,sales.* from products join sales on products.name=sales.name ;

select p.*,s.* from products p left outer join sales s on p.name=s.name ;

select p.*,s.* from products p right outer join sales s on p.name=s.name ;

select p.*,s.* from products p full outer join sales s on p.name=s.name ;

select p.* from products p left semi join sales s on p.name=s.name ;

select /* +mapjoin(sales) */ products.*,sales.* from products join sales on products.name=sales.name ;

select /* +mapjoin(sales) */ products.*,sales.* from products left outer join sales on products.name=sales.name ;

select /* +mapjoin(products) */ products.*,sales.* from products right outer join sales on products.name=sales.name ;


create table if not exists student_textfile(name string,id int,course string,year int)
stored as textfile ;

create table if not exists student_sequencefile(name string,id int,course string,year int)
stored as sequencefile ;

create table if not exists student_avro(name string,id int,course string,year int)
stored as avro ;

create table if not exists student_rcfile(name string,id int,course string,year int)
stored as rcfile ;

create table if not exists student_orc(name string,id int,course string,year int)
stored as orc ;

create table if not exists student_parquet(name string,id int,course string,year int)
stored as parquet ;

insert into table student_textfile select * from student ;

insert into table student_sequencefile select * from student ;

insert into table student_avro select * from student ;

insert into table student_rcfile select * from student ;

insert into table student_orc select * from student ;

insert into table student_parquet select * from student ;

create table if not exists student_acid(name string,id int,course string,year int)
clustered by (name) into 4 buckets 
stored as orc
tblproperties('transactional'='true')
;

insert into table student_acid select * from student ;

select * from student_acid tablesample(bucket 1 out of 4 on name) ;





------------------Theory--------------------
1.Hive is developed by Facebook
2.hive is a dataware house not a database 
3.hive data is totally denormalised 
4.hive is meant only for analytical purpose not for transactional purpose 
5.hive query lang is similar to sql 
6.hive always needs metastore db like mysql 
7.data models of hive 
 a)Tables 
 b)Partitions 
 c)Bucketisation
8.DATA TYPES IN HIVE 
   tinyint,smallint,int,bigint,long,float,double,string,varchar,char,date,timestamp,boolean,binary -->Primitive
   array,map,struct,uniontype ----------------------------------------------------------------------->complex 
   array<data-type> 
   map<primitive,data-type>
9.serde
10.In hive there is no concept of primary key and foreign key 
11.default fields terminated by '\001' or ctrl+A 
12.default collection items terminated by '\002' or ctrl+B
13.default map keys terminated by '\003' or ctrl+C 
14.default lines terminated by '\n' 
15.default database name id default 
16.default metastore db name is embed derby 
17.hive is one of the hadoop eco system tool for processing the data the engine in hive is MR(map reduce) 
18.Actual data is stored in HDFS and metadata is stored in RDBMS 
19.To start hive hdfs to be started and RDBMS to be running 
20.diff b/w managed and external table 
  In hive we have two types of tables 
    1)MANAGED Tables
    2)EXTERNAL TABLE 
       evn temporary table comes under MANAGED TABLE only 
we can't see any diff when we are performing insert/load and select query 
only at the time of dropping a table we can see the difference 
  when we drop an managed table the metadata and actualdata will lost 
  when we drop an external table the metadata in RDBMS will be lost but actualdata in hdfs will still present 

21.hadoop properties 
   hive properties 
   custom properties 
   environment properties 
   system properties 
 22.Installtion
    a)INTERNAL/LOCAL : Only one session will be available 
    b)EXTERNAL/REMOTE :Multiple sessions will be available one session will be independent of other 
       if we want to do any changes change in hive-site.xml 
23.Partitions 
 At the time of table creation time there is no concept of static or dynamic partition table it is a table only 
 when we are loading/inserting the data into table we will discuss static or dynamic partition table 
  if we are using static approach then it is called static partition table 
  if we are using dynamic approach then it is called dynamic partition table 
In dynamic partition there are 2 modes 
  1)STRICT MODE (default) atleast one static partition column
  2)NON STRICT MODE (no condition on the partition column) 

use load statement if all partition columns are static data always from file only
use insert statement no condition on the partition columns but data always from table only 

use static approach if we know all partition columns are all static 
use dynamic approach if we don't know the partiton columns 

order of partition columns 
 1)s1,s2,s3,s4,s5,s6 
 2)s1,s2,s3,d1,d2,d3
 3)d1,d2,d3,d4,d5,d6 
 4)d1,d2,d3,s1,s2,s3 (not allowed)

The parent of the static partition column should not dynamic 

Order of the partition columns are very importnat 

we can't change the order 

partiton can be used for both static and dynamic data 

Bucketing main purpose to partiton and sampling the data 

the problems in partition can be overcomed here 

let us take three partitions one having very hign amount of data second having very low data and third having medium 
 in low data it is unnecessay burden on the name node lot's of small files is the issue 

data skewness can be overcomed with  bucketisation and skewjoin also 

buckets are fixed 

we can't change the bucket number later 

bucketing is used for only static data 

by default join are reduce side joins 

when we use map side joins in distributed cache place the small table 
  because small table comparision with big table is much more easier then big table comparision with small table 

SERRDE:

methods :
1)initialise:Initialising the job resources 
2)serialise: serialising the data 
3)desearialise:deserialising the data 

At the time of table creation time serde will automatically initialise and validate the schema 

when loading/inderting the data into table table data will be serialised 

when doing select query the data will be deserialised 

default serde is lazy simple serde 
default input format textinput format 
default output format hiveignoekeytextoutput format 

In older serde accepts only string data types 

'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'

In new serde accepts primitive datatypes 

'org.apache.hadoop.hive.serde2.RegexSerDe'

HDFS doesn't allow updates only appends 

If hive want to support updates 
  table must and should be bucketised 
  stored as orc 
  tblproperties('transactional'='true')
  we can't update the bucketed column it is like a primary key 

hive doen't support updates orc supports updates 

Storage options 
textfile 
sequencefile
avro
rcfile
orc
parquet 


avro is good for doing transactional 
parquet is good for analytical purpose 


why parquet?
 data is high secure 
 data storing and retrival time is very fast 
 support from apache contributer for parquet is good 

One table is stored as textfile and Another table is stored as parquet can we do join operations ?
Yes 

ISSUES:

1)COMPATIBLE ISSUES hadoop2.6 and hive1.2 good 
2)we have to reduce the sub queries 
3)heap space issues 
  xmx-xsx 
4)we havve to reduce the complexity of the query's 



-----------------------HOW to handle nulls in hive
In tblproperties("serilization.null.format"="")


hive> select * from siva;
OK
ram     NULL
NULL    2
ramu    3
Time taken: 0.066 seconds, Fetched: 3 row(s)
hive> alter table siva set tblproperties("serialization.null.format"="");
OK
Time taken: 0.31 seconds
hive> select * from siva;
OK
ram     NULL
\N      2
ramu    3
Time taken: 0.055 seconds, Fetched: 3 row(s)
hive> alter table siva set tblproperties("serialization.null.format"=" ");
OK
Time taken: 0.51 seconds
hive> select * from siva;
OK
ram     NULL
\N      2
ramu    3
Time taken: 0.063 seconds, Fetched: 3 row(s)
hive> alter table siva set tblproperties("serialization.null.format"="siva");
OK
Time taken: 0.301 seconds
hive> select * from siva;
OK
ram     NULL
\N      2
ramu    3
Time taken: 0.13 seconds, Fetched: 3 row(s)
hive>

